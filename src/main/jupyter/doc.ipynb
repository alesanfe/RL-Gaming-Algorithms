{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Clase Game."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta clase representa un juego y proporciona métodos para resolverlo utilizando diferentes algoritmos de aprendizaje por refuerzo, como Montecarlo, Q-Learning y Sarsa. Permite establecer el entorno del juego, el factor de descuento, el factor de aprendizaje y el número de iteraciones. También calcula y muestra estadísticas del entorno, como la recompensa media, la longitud media de los episodios y el tiempo de ejecución."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Método para resolver por montercarlo (resolve_by_montecarlo)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método resuelve el entorno del juego utilizando el algoritmo de Montecarlo con inicios exploratorios. Realiza un entrenamiento del agente durante un número determinado de iteraciones y devuelve el agente entrenado. También registra el tiempo de ejecución del algoritmo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Método para resolver por Q-Learning (resolve_by_q_learning)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método resuelve el entorno del juego utilizando el algoritmo de Q-Learning. Recibe como parámetro el valor de epsilon para la política epsilon-voraz. Realiza un entrenamiento del agente durante un número determinado de iteraciones y devuelve el agente entrenado. También registra el tiempo de ejecución del algoritmo.\n",
    "\n",
    "Parámetro:\n",
    "\n",
    "- *epsilon* (tipo: float): El valor de epsilon para la política epsilon-voraz utilizada en el algoritmo Q-Learning. Controla la exploración vs explotación del agente durante el entrenamiento. Un valor más alto de epsilon favorece la exploración de nuevas acciones, mientras que un valor más bajo favorece la explotación de las acciones aprendidas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3. Método para resolver por Sarsa (resolve_by_montecarlo)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método resuelve el entorno del juego utilizando el algoritmo de Sarsa. Recibe como parámetros los valores de epsilon, alpha y gamma para la política epsilon-voraz. Realiza un entrenamiento del agente durante un número determinado de iteraciones y devuelve el agente entrenado. También registra el tiempo de ejecución del algoritmo.\n",
    "\n",
    "Parámetros:\n",
    "\n",
    "- *epsilon* (tipo: float): El valor de epsilon para la política epsilon-voraz utilizada en el algoritmo Sarsa.\n",
    "- *alpha* (tipo: float): El factor de aprendizaje que controla la influencia de las nuevas experiencias en las actualizaciones de las estimaciones de acción-valor.\n",
    "- *gamma* (tipo: float): El factor de descuento que determina la importancia de las recompensas futuras en comparación con las recompensas inmediatas. Un valor más alto de gamma hace que el agente tenga en cuenta más las recompensas futuras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4. Método para mostrar estadísticas (print_stats)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método imprime las estadísticas del entorno del juego. Muestra el tiempo de ejecución, la recompensa media, la longitud media de los episodios, el número de episodios, la máxima y mínima recompensa alcanzada, el número de episodios completados exitosamente, el porcentaje de éxito, el promedio de recompensa de los episodios exitosos y el promedio de recompensa de los episodios fallidos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Clase GameComparator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método imprime las estadísticas del entorno del juego. Muestra el tiempo de ejecución, la recompensa media, la longitud media de los episodios, el número de episodios, la máxima y mínima recompensa alcanzada, el número de episodios completados exitosamente, el porcentaje de éxito, el promedio de recompensa de los episodios exitosos y el promedio de recompensa de los episodios fallidos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. Método para comparar algoritmos (compare_different_algorithms)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método compara los diferentes algoritmos de aprendizaje por refuerzo utilizados para resolver el juego. Recibe como parámetros una lista de algoritmos a comparar, el valor de epsilon para la política epsilon-voraz, el valor de alpha y el valor de gamma. Ejecuta cada algoritmo en el juego y recopila las estadísticas. Luego imprime los resultados de la comparación, mostrando los mejores y peores valores para cada estadística."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Método para comparar distintos casos (compare_different_cases)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método compara diferentes casos de resolución del juego utilizando un algoritmo específico. Recibe como parámetros el algoritmo a comparar, el valor de epsilon para la política epsilon-voraz, el valor de alpha y el valor de gamma. Ejecuta el algoritmo en el juego con diferentes casos y recopila las estadísticas. Luego imprime los resultados de la comparación, mostrando los mejores y peores valores para cada estadística."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3. Método para comparar distintos entornos (compare_different_environments)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método compara diferentes entornos del juego utilizando un algoritmo específico y valores de parámetros específicos. Recibe como parámetros el algoritmo a comparar, el valor de epsilon para la política epsilon-voraz, el valor de alpha y el valor de gamma. Crea diferentes instancias del juego con diferentes entornos y resuelve cada instancia con el algoritmo y los parámetros dados. Luego imprime los resultados de la comparación, mostrando los mejores y peores valores para cada estadística."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Clase Sarsa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta clase implementa el algoritmo de aprendizaje por refuerzo SARSA. Utiliza una tabla Q para almacenar y actualizar los valores de acción-valor para cada estado y acción en el entorno del juego. El algoritmo entrena al agente durante un número determinado de episodios y recopila estadísticas del entorno."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los datos que se le deberán de proporcionar serán los siguientes:\n",
    "- *env* (tipo: objeto): El entorno en el que se ejecuta el algoritmo.\n",
    "- *discount_factor* (tipo: float): El factor de descuento que determina la importancia de las recompensas futuras en comparación con las recompensas inmediatas.\n",
    "- *learning_factor* (tipo: float): El factor de aprendizaje que controla la influencia de las nuevas experiencias en las actualizaciones de las estimaciones de acción-valor.\n",
    "- *export_policy* (tipo: objeto): La política de exportación utilizada para elegir acciones en función de los valores de acción-valor almacenados en la tabla Q.\n",
    "\n",
    "Además se utilizan los siguientes datos:\n",
    "- *statistics*: Una instancia de la clase EnvironmentStatistic que se utiliza para recopilar y calcular las estadísticas del entorno.\n",
    "- *q_table*: La tabla Q que almacena los valores de acción-valor para cada estado y acción en el entorno del juego."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Inicialización de tabla que contiene los valores q para cada estado y acción (initialize_q_table)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicializa la tabla Q con valores aleatorios. La tabla Q es una matriz de tamaño (número de estados x número de acciones) y se utiliza para almacenar los valores de acción-valor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. Actualización de tabla que contiene los valores q para cada estado y acción (update_q_table)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Actualiza el valor de una acción para un estado dado utilizando la regla de actualización de SARSA.\n",
    "\n",
    "Argumentos:\n",
    "\n",
    "- *state* (tipo: int): El estado actual.\n",
    "- *action* (tipo: int): La acción tomada en el estado actual.\n",
    "- *reward* (tipo: float): La recompensa recibida por tomar la acción en el estado actual.\n",
    "- *next_state* (tipo: int): El siguiente estado resultante de tomar la acción.\n",
    "- *next_action* (tipo: int): La siguiente acción elegida en el siguiente estado."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3. Método para elegir una acción (choose_action)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Elige una acción para un estado dado utilizando la política de exportación.\n",
    "\n",
    "Argumentos:\n",
    "\n",
    "- *state* (tipo: int): El estado actual.\n",
    "- *info* (tipo: objeto): Información adicional necesaria para tomar decisiones, proporcionada por el entorno."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Método para ejecutar un episodio (execute_episode)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejecuta un episodio completo en el entorno del juego. El episodio comienza en el estado proporcionado por el método reset() del entorno y continúa hasta que se obtenga una señal de terminación o truncado. Durante el episodio, el algoritmo elige acciones, actualiza la tabla Q y recopila estadísticas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Método para entrenar al agente (train)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejecuta el algoritmo SARSA durante un número determinado de episodios.\n",
    "\n",
    "Argumentos:\n",
    "\n",
    "- *num_episodes* (tipo: int): El número de episodios a entrenar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5. Método para obtener las estadísticas del entorno (calculate_statistics)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calcula las estadísticas a partir de los datos de los episodios recopilados durante el entrenamiento. Las estadísticas incluyen la recompensa media, la longitud media de los episodios y otras métricas relevantes del entorno."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
